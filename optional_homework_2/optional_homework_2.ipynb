{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf6DUdqvMr2z"
      },
      "source": [
        "# Optional Homework 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lNWDh5S8zjw"
      },
      "source": [
        "In a new python environment with python>=3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sDlD3KX8zjy",
        "outputId": "c224a81c-e2db-4dd4-e165-0c117cbbb449"
      },
      "outputs": [],
      "source": [
        "!pip install \"torch_uncertainty[image] @ git+https://github.com/ENSTA-U2IS-AI/torch-uncertainty@dev\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dcLeda8S8zjz"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "batch_size = 10\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 2e-4\n",
        "lr_decay_epochs = 20\n",
        "lr_decay = 0.1\n",
        "nb_epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217,
          "referenced_widgets": [
            "e7ac31001fce42f9a6c1742b2f0f817a",
            "a8f05cc5a1f346be9145fc41b17bf126",
            "f8cdcc7bce40484d908b90e6f90649b5",
            "9cdbef89c3ca4f2980c7d751b87c9fd1",
            "861df32105834f6294b844b55d2fc940",
            "6bdc7a31482041e595b67c4153f28b29",
            "73f9e4ca64024f25bae74a5e924a9f18",
            "b7d52bf1bf494b96946d805b27710e69",
            "5c4e318aa6fd43bbbc21cc67dd3f139c",
            "80b2eeb6e2d144b280012dfee12c69fe",
            "7c4c90cbbf154109be1e6976efae0b6f",
            "9208a5b5146a4f7486a3457471efcbc2",
            "790520797f32449c8a6b27edde85f5c7",
            "aaa51c3b805f4716bf0871d2332b9341",
            "7b6ea7859ce64f9b8b0f450e68781405",
            "6e364c9177a54618944edad4ae8b40d5",
            "b3a8d6d9eb3543d48ac1b5d7d6f88003",
            "7d10b247927b4cfc9ab31413f79fdedc",
            "f54ed6337d8746fb8e89665502fbdec8",
            "152d79f29c4c4cfdbb26e16e1a530b83",
            "be073e0667e5437e8968d298b2f2fcca",
            "d52357b30b5247d1aad718493ad53fac",
            "a4d307edd2834f72b5c2d4eff21042d0",
            "f97aa393955043a1810f7c3dc807a9fe",
            "f377338154384c08a9b620857cc1fecf",
            "35f7068931aa4f6687d6de866f16d55a",
            "3fd6d7a73aeb4aabb9f103d482753ba5",
            "0d94a0a0e0d540d0841a29fb60122e7b",
            "7b40dd48f7cd4beab8125638f592d1ed",
            "98cc66c85f9244d3bb5e1e426a640cad",
            "56c1a7855c4d4163bc3e4ad0bb503898",
            "8adce99bd93e417b9eeeec7d0981bbbf",
            "adc183b26a87432ab9c28a4c8fac2df7"
          ]
        },
        "id": "6TqJpiUX8zjz",
        "outputId": "4f8f2d8f-ef07-4f74-bfc1-1be7dfceb563"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from einops import rearrange\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms import v2\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "\n",
        "from torch_uncertainty.datasets import MUAD\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_transform = v2.Compose(\n",
        "    [\n",
        "        v2.Resize(size=(256, 512), antialias=True),\n",
        "        v2.RandomHorizontalFlip(),\n",
        "        v2.ToDtype(\n",
        "            dtype={\n",
        "                tv_tensors.Image: torch.float32,\n",
        "                tv_tensors.Mask: torch.int64,\n",
        "                \"others\": None,\n",
        "            },\n",
        "            scale=True,\n",
        "        ),\n",
        "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transform = v2.Compose(\n",
        "    [\n",
        "        v2.Resize(size=(256, 512), antialias=True),\n",
        "        v2.ToDtype(\n",
        "            dtype={\n",
        "                tv_tensors.Image: torch.float32,\n",
        "                tv_tensors.Mask: torch.int64,\n",
        "                \"others\": None,\n",
        "            },\n",
        "            scale=True,\n",
        "        ),\n",
        "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_set = MUAD(root=\"./data\", target_type=\"semantic\", version=\"small\", split=\"train\", transforms=train_transform, download=True)\n",
        "val_set = MUAD(root=\"./data\", target_type=\"semantic\", version=\"small\", split=\"val\", transforms=val_transform, download=True)\n",
        "test_set = MUAD(root=\"./data\", target_type=\"semantic\", version=\"small\", split=\"test\", transforms=val_transform, download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9qKz4D38zj0"
      },
      "source": [
        "Let us see the first sample of the validation set. The first image is the input and the second image is the target (ground truth)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rgOIk4B8zj0",
        "outputId": "0ffe654a-6312-45dd-e0f7-828f459e78e9"
      },
      "outputs": [],
      "source": [
        "sample = train_set[0]\n",
        "img, tgt = sample\n",
        "img.size(), tgt.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8mlTc2u8zj1"
      },
      "source": [
        "Visualize a validation input sample (and RGB image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "P5EF4vBD8zj1",
        "outputId": "3b60eefe-4daa-43c7-c884-9c9a1c384a5e"
      },
      "outputs": [],
      "source": [
        "# Undo normalization on the image and convert to uint8.\n",
        "mean = torch.tensor([0.485, 0.456, 0.406], device=img.device)\n",
        "std = torch.tensor([0.229, 0.224, 0.225], device=img.device)\n",
        "img = img * std[:, None, None] + mean[:, None, None]\n",
        "img = F.to_dtype(img, torch.uint8, scale=True)\n",
        "F.to_pil_image(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyKaF3mu8zj1"
      },
      "source": [
        "Visualize the same image above but segmented (our goal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "YgTqq-hN8zj2",
        "outputId": "eb0b6348-09cd-4e8f-ab8e-d14758bd862b"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import draw_segmentation_masks\n",
        "\n",
        "tmp_tgt = tgt.masked_fill(tgt == 255, 21)\n",
        "tgt_masks = tmp_tgt == torch.arange(22, device=tgt.device)[:, None, None]\n",
        "img_segmented = draw_segmentation_masks(img, tgt_masks, alpha=1, colors=val_set.color_palette)\n",
        "F.to_pil_image(img_segmented)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K21284qq8zj2"
      },
      "source": [
        "Below is the complete list of classes in MUAD, presented as:\n",
        "\n",
        "1.   Class Name\n",
        "2.   Train ID\n",
        "3.   Segmentation Color in RGB format [R,G, B]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CBuq3FL8zj3",
        "outputId": "8807aa86-2061-4d25-ec9a-dbb0206a0d69"
      },
      "outputs": [],
      "source": [
        "for muad_class in train_set.classes:\n",
        "    class_name = muad_class.name\n",
        "    train_id = muad_class.id\n",
        "    color = muad_class.color\n",
        "    print(f\"Class: {class_name:35} | Train ID: {train_id:2} | Color: {color}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM6QQ8GP8zj3"
      },
      "source": [
        "Here is a more comprhensive review of the diffrent classes :\n",
        "(training non-labeled data will use train ID 21 and not 255)\n",
        "\n",
        "\n",
        "| **class names**                       | **ID** |\n",
        "|----------------------------------------|---------|\n",
        "| road                                   | 0       |\n",
        "| sidewalk                               | 1       |\n",
        "| building                               | 2       |\n",
        "| wall                                   | 3       |\n",
        "| fence                                  | 4       |\n",
        "| pole                                   | 5       |\n",
        "| traffic light                          | 6       |\n",
        "| traffic sign                           | 7       |\n",
        "| vegetation                             | 8       |\n",
        "| terrain                                | 9       |\n",
        "| sky                                    | 10      |\n",
        "| person                                 | 11      |\n",
        "| rider                                  | 12      |\n",
        "| car                                    | 13      |\n",
        "| truck                                  | 14      |\n",
        "| bus                                    | 15      |\n",
        "| train                                  | 16      |\n",
        "| motorcycle                             | 17      |\n",
        "| bicycle                                | 18      |\n",
        "| bear deer cow                          | 19      |\n",
        "| garbage_bag stand_food trash_can       | 20      |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IBM8fzf8zj3"
      },
      "source": [
        "We will feed the DNN the first raw image of the road view and as target it will be the dark image below and not the colored one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "wU4A6w428zj3",
        "outputId": "718ae850-e5c2-4ce6-cc1d-b58183bb7cf6"
      },
      "outputs": [],
      "source": [
        "tgt_img = F.to_pil_image(F.to_dtype(tgt, torch.uint8))\n",
        "tgt_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLRSaJgV8zj3",
        "outputId": "c28f1870-9fad-4110-fc3f-66ff86e54c3d"
      },
      "outputs": [],
      "source": [
        "tgt_img.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "I645jBROgz0S",
        "outputId": "0dabcc99-ddbe-4236-fe3e-10b8ea8baa33"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.array(tgt_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "BarQp0lghrLZ",
        "outputId": "6709f47e-851f-47a7-e52f-ac3f7b48b091"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(tgt_img)\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "stBYNFhkiqJz",
        "outputId": "b57fea13-44f4-4de7-f361-fe4387eebbe8"
      },
      "outputs": [],
      "source": [
        "test = np.array(np.array(tgt_img) == 255)\n",
        "plt.imshow(test, cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdF1nx6T8zj4"
      },
      "source": [
        "**Q2/ Why is the target image dark and what's the bright part? (hint: print the numpy array)**\n",
        "\n",
        "The target image corresponds to class labels. So the image contains integers representing the class attributed to each pixel. The bright areas on the image correspond to unlabeled pixels (pixel with no specific class associated) that are set to the value 255 by default."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXbrvwC98zj4"
      },
      "source": [
        "**Q3/ Please study the dataset a bit. What it is about?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "9TXIff1KjcCc",
        "outputId": "e5e31b82-fbf2-4e20-c684-2535340b2882"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "n_images = 4\n",
        "denormalize = True\n",
        "segment = True\n",
        "fig, axs = plt.subplots(n_images, 2, figsize=(8, n_images*2))\n",
        "\n",
        "for i in range(n_images):\n",
        "    id = random.randint(0, len(train_set)-1)\n",
        "\n",
        "    img, target = train_set[id]\n",
        "\n",
        "    if denormalize:\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406], device=img.device)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225], device=img.device)\n",
        "        img = img * std[:, None, None] + mean[:, None, None]\n",
        "    img = F.to_dtype(img, torch.uint8, scale=True)\n",
        "    img = F.to_pil_image(img)\n",
        "\n",
        "    if segment:\n",
        "        tmp_target = target.masked_fill(target == 255, 21)\n",
        "        target_masks = tmp_target == torch.arange(22, device=target.device)[:, None, None]\n",
        "        img_segmented = draw_segmentation_masks(train_set[id][0], target_masks, alpha=1, colors=val_set.color_palette)\n",
        "        target = F.to_pil_image(img_segmented)\n",
        "    else:\n",
        "        target = F.to_dtype(target, torch.uint8, scale=True)\n",
        "        target = F.to_pil_image(target)\n",
        "\n",
        "    axs[i,0].imshow(img)\n",
        "    axs[i,0].axis(\"off\")\n",
        "    axs[i,1].imshow(target)\n",
        "    axs[i,1].axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXcgwODZmT2l"
      },
      "source": [
        "This dataset contains pair of images. The first image correponds to photos from a camera fixed at the front of a car in different situations. It seems that the environments in which the car evolves are mostly cities (roads, buildings, pedestrians, cars, trees...). The second \"image\" contains the label of each pixel from the first image: a single value is attributed to all pixels that are part of an object (roads, buildings, pedestrians, cars, trees...). If an object is not in the predefined list of recognized object a default value is attributed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ivjRh97e8zj4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 2\n",
        "\n",
        "train_loader = DataLoader(\n",
        "        train_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers)\n",
        "\n",
        "val_loader = DataLoader(\n",
        "        val_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "        test_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Zn9_b_KT8zj4"
      },
      "outputs": [],
      "source": [
        "def enet_weighing(dataloader, num_classes, c=1.02):\n",
        "    \"\"\"Computes class weights as described in the ENet paper.\n",
        "\n",
        "        w_class = 1 / (ln(c + p_class)),\n",
        "\n",
        "    where c is usually 1.02 and p_class is the propensity score of that\n",
        "    class:\n",
        "\n",
        "        propensity_score = freq_class / total_pixels.\n",
        "\n",
        "    References:\n",
        "        https://arxiv.org/abs/1606.02147\n",
        "\n",
        "    Args:\n",
        "        dataloader (``data.Dataloader``): A data loader to iterate over the\n",
        "            dataset.\n",
        "        num_classes (``int``): The number of classes.\n",
        "        c (``int``, optional): AN additional hyper-parameter which restricts\n",
        "            the interval of values for the weights. Default: 1.02.\n",
        "\n",
        "    \"\"\"\n",
        "    class_count = 0\n",
        "    total = 0\n",
        "    for _, label in dataloader:\n",
        "      label = label.cpu().numpy()\n",
        "      # Flatten label\n",
        "      flat_label = label.flatten()\n",
        "      flat_label = flat_label[flat_label != 255]\n",
        "\n",
        "      # Sum up the number of pixels of each class and the total pixel\n",
        "      # counts for each label\n",
        "      class_count += np.bincount(flat_label, minlength=num_classes)\n",
        "      total += flat_label.size\n",
        "\n",
        "    # Compute propensity score and then the weights for each class\n",
        "    propensity_score = class_count / total\n",
        "    return 1 / (np.log(c + propensity_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfrMULrG8zj4",
        "outputId": "6dc06da8-2b2b-4be6-bf3d-44f010238db0"
      },
      "outputs": [],
      "source": [
        "print(\"Computing class weights...\")\n",
        "print(\"(this can take a while depending on the dataset size)\")\n",
        "class_weights = enet_weighing(train_loader, 22)\n",
        "class_weights = torch.from_numpy(class_weights).float().cuda()\n",
        "print(\"Class weights:\", class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98-kLO-_lPiz",
        "outputId": "2b66ff49-39a8-44ed-a7c2-8dcebcf92d4c"
      },
      "outputs": [],
      "source": [
        "for i, muad_class in enumerate(train_set.classes):\n",
        "    class_name = muad_class.name\n",
        "    train_id = muad_class.id\n",
        "    print(f\"Class: {class_name:35} | Train ID: {train_id:2} | Weight: {class_weights[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK5m9TzzlmwX"
      },
      "source": [
        "The weight attributed to each class seems coherent. Very frequent classes like road (that take a lot of space on the images) have a small weight whereas less frequent classes have a higher weight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcyhmI8m8zj5"
      },
      "source": [
        "**Q4/ Why do we need to evaluate the class_weights?**\n",
        "\n",
        "Notice that the more the class is frequent, the smaller its weight is. These weights are used to balance classes in the dataset. For instance, in the dataset there are way more pixels of road than pixels of trees. Without using class weights to balance classes, the model might learn not to predict classes that are less probable/represented in the data because they do not impact the loss a lot. In the worst case scenario: the model learns to predict only the most frequent class in the dataset because it is the correct prediction most of the times..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tkKjz_k8zj5"
      },
      "source": [
        "## C. building the DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Dn_vEBFM8zj5"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(conv => BN => ReLU) * 2.\"\"\"\n",
        "\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class InConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv = DoubleConv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.mpconv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_ch, out_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mpconv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super().__init__()\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2)\n",
        "\n",
        "        self.conv = DoubleConv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        if self.bilinear:\n",
        "            x1 = F.resize(x1, size=[2*x1.size()[2],2*x1.size()[3]],\n",
        "                          interpolation=v2.InterpolationMode.BILINEAR)\n",
        "        else:\n",
        "            x1 = self.up(x1)\n",
        "\n",
        "        # input is CHW\n",
        "        diff_y = x2.size()[2] - x1.size()[2]\n",
        "        diff_x = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diff_x // 2, diff_x - diff_x // 2,\n",
        "                        diff_y // 2, diff_y - diff_y // 2])\n",
        "\n",
        "        # for padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    # Please note that we have added dropout layer to be abble to use MC dropout\n",
        "    def __init__(self, classes):\n",
        "        super().__init__()\n",
        "        self.inc = InConv(3, 32)\n",
        "        self.down1 = Down(32, 64)\n",
        "        self.down2 = Down(64, 128)\n",
        "        self.down3 = Down(128, 256)\n",
        "        self.down4 = Down(256, 256)\n",
        "        self.up1 = Up(512, 128)\n",
        "        self.up2 = Up(256, 64)\n",
        "        self.up3 = Up(128, 32)\n",
        "        self.up4 = Up(64, 32)\n",
        "        self.dropout = nn.Dropout2d(0.1)\n",
        "        self.outc = OutConv(32, classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.dropout(x)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.dropout(x)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.dropout(x)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.dropout(x)\n",
        "        return self.outc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWgXo2w2QrmA"
      },
      "source": [
        "**Q5/ Do we really use U-Net? What did I change? (that is hard)**\n",
        "\n",
        "The architecture looks very similar from the original U-Net architecture proposed in [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597). However there are some small differences. The number of channels in not the same (generally half of what was originally proposed). According to the same paper, there is no batch normalisation. The comment in the code snippet says dropout layers are only here to use Monte-Carlo dropout for model uncertainty evaluation.\n",
        "\n",
        "**Q6/Do we need a backbone with U-Net?**\n",
        "\n",
        "With U-Net using a backbone is not necessary. However, it is possible to use a backbone for the encoder part to improve the overall model performance. No backbone is used in this practical work, the U-Net model is trained from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9xW-iRN8zj6"
      },
      "source": [
        "## D. Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4n2-8wIB8zj6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Colors from Colorbrewer Paired_12\n",
        "colors = [[31, 120, 180], [51, 160, 44]]\n",
        "colors = [(r / 255, g / 255, b / 255) for (r, g, b) in colors]\n",
        "\n",
        "def plot_losses(train_history, val_history):\n",
        "    x = np.arange(1, len(train_history) + 1)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, train_history, color=colors[0], label=\"Training loss\", linewidth=2)\n",
        "    plt.plot(x, val_history, color=colors[1], label=\"Validation loss\", linewidth=2)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.title(\"Evolution of the training and validation loss\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_accu(train_history, val_history):\n",
        "    x = np.arange(1, len(train_history) + 1)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, train_history, color=colors[0], label=\"Training MIoU\", linewidth=2)\n",
        "    plt.plot(x, val_history, color=colors[1], label=\"Validation MIoU\", linewidth=2)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Mean IoU\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.title(\"Evolution of MIoU\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0gLFWkM8zj7"
      },
      "source": [
        "**Q7/ What is the IoU?**\n",
        "\n",
        "IoU stands for Intersection over Union, which is a metric commonly used to evaluate the performance of object detection models. It measures the overlap between two bounding boxes: the predicted bounding box (from the model) and the ground truth bounding box (the actual object location).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWD37D48zj7"
      },
      "source": [
        "### Training function\n",
        "\n",
        "**Q8/ Please complete the training and the test function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5ETHIIe68zj7"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.utilities.compute import _safe_divide\n",
        "\n",
        "def train(model, data_loader, optim, criterion, metric, iteration_loss=False):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    metric.reset()\n",
        "    for step, batch_data in enumerate(data_loader):\n",
        "        # Get the inputs and labels\n",
        "        img = batch_data[0].cuda()\n",
        "        labels = batch_data[1].cuda()\n",
        "\n",
        "        # Forward propagation\n",
        "        logits = model(img)\n",
        "        flatten_logits = rearrange(logits, \"b c h w -> (b h w) c\")\n",
        "        flatten_labels = labels.flatten()\n",
        "\n",
        "        valid_mask = flatten_labels != 255\n",
        "\n",
        "        # Loss computation\n",
        "        loss = criterion(flatten_logits[valid_mask], flatten_labels[valid_mask])\n",
        "\n",
        "        # Backpropagation\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        # Keep track of loss for current epoch\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Keep track of the evaluation metric\n",
        "        metric.update(flatten_logits[valid_mask].detach(), flatten_labels[valid_mask].detach())\n",
        "\n",
        "        if iteration_loss:\n",
        "            print(\"[Step: %d] Iteration loss: %.4f\" % (step, loss.item()))\n",
        "\n",
        "    # Compute IoU per class\n",
        "    tp, fp, _, fn = metric._final_state()\n",
        "    iou_per_class = _safe_divide(tp, tp + fp + fn, zero_division=float(\"nan\"))\n",
        "\n",
        "    return epoch_loss / len(data_loader), iou_per_class, metric.compute().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiMNyCRY8zj8"
      },
      "source": [
        "### Validation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "S_UTKVIP8zj8"
      },
      "outputs": [],
      "source": [
        "def test(model, data_loader, criterion, metric, iteration_loss=False):\n",
        "    model.eval()\n",
        "    epoch_loss = 0.0\n",
        "    metric.reset()\n",
        "    for step, batch_data in enumerate(data_loader):\n",
        "        # Get the inputs and labels\n",
        "        img = batch_data[0].cuda()\n",
        "        labels = batch_data[1].cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Forward propagation\n",
        "            logits = model(img)\n",
        "            flatten_logits = rearrange(logits, \"b c h w -> (b h w) c\")\n",
        "            flatten_labels = labels.flatten()\n",
        "\n",
        "            valid_mask = flatten_labels != 255\n",
        "\n",
        "            # Loss computation\n",
        "            loss = criterion(flatten_logits[valid_mask], flatten_labels[valid_mask])\n",
        "\n",
        "        # Keep track of loss for current epoch\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Keep track of evaluation the metric\n",
        "        metric.update(flatten_logits[valid_mask], flatten_labels[valid_mask])\n",
        "\n",
        "        if iteration_loss:\n",
        "            print(\"[Step: %d] Iteration loss: %.4f\" % (step, loss.item()))\n",
        "\n",
        "    # Compute IoU per class\n",
        "    tp, fp, _, fn = metric._final_state()\n",
        "    iou_per_class = _safe_divide(tp, tp + fp + fn, zero_division=float(\"nan\"))\n",
        "\n",
        "    return epoch_loss / len(data_loader), iou_per_class, metric.compute().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiwqqSLU8zj8"
      },
      "source": [
        "## E. Training Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzAQQAS98zj9"
      },
      "source": [
        "**Q9/ Please train your DNN and comment**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "p3QBxf8N8zj9",
        "outputId": "17b75b42-c6f6-4938-ddbc-716e8873f4de"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from torch.optim import lr_scheduler\n",
        "from torch_uncertainty.metrics.classification import MeanIntersectionOverUnion\n",
        "\n",
        "def save_checkpoint(\n",
        "        checkpoint, is_best, path=\"checkpoint.pth.tar\",\n",
        "        best_path=\"checkpoint_best.pth.tar\"):\n",
        "    torch.save(checkpoint, path)\n",
        "    if is_best:\n",
        "        shutil.copyfile(path, best_path)\n",
        "\n",
        "# Intialize U-Net\n",
        "n_classes = 22\n",
        "net = UNet(n_classes).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_epochs, gamma=lr_decay)\n",
        "# We are going to use the CrossEntropyLoss loss function as it's most\n",
        "# frequentely used in classification problems with multiple classes which\n",
        "# fits the problem. This criterion combines LogSoftMax and NLLLoss.\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "metric = MeanIntersectionOverUnion(n_classes).to(device) # ignore_index=21???\n",
        "\n",
        "# Run training and track with wandb\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "train_miou_history = []\n",
        "val_miou_history = []\n",
        "best_loss = float(\"inf\") # Best loss\n",
        "# nb_epochs = 2 # FOR TESTING PURPOSE ONLY\n",
        "for epoch in range(1, nb_epochs+1):\n",
        "    print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "\n",
        "    # Train one epoch\n",
        "    train_loss, iou_per_class, train_metric = train(net, train_loader, optimizer, criterion, metric, iteration_loss=False)\n",
        "    train_loss_history.append(train_loss)\n",
        "    train_miou_history.append(train_metric)\n",
        "\n",
        "    # Validation\n",
        "    val_loss, iou_per_class, val_metric = test(net, val_loader, criterion, metric, iteration_loss=False)\n",
        "    val_loss_history.append(val_loss)\n",
        "    val_miou_history.append(val_metric)\n",
        "\n",
        "    # Learning rate scheduler step\n",
        "    lr_scheduler.step(val_loss)\n",
        "\n",
        "    # Save model\n",
        "    is_best = val_loss < best_loss\n",
        "    best_loss = min(val_loss, best_loss)\n",
        "    save_checkpoint(net.state_dict(), is_best)\n",
        "\n",
        "    # Logging\n",
        "    if (epoch % 10) == 0:\n",
        "        print(\n",
        "            f\"[Epoch {epoch:2}/{nb_epochs}]\"\n",
        "            f\"Train: loss={train_loss:.3f}, metric={train_metric:.3f} |\"\n",
        "            f\"Validation: loss={val_loss:.3f}, metric={val_metric:.3f}\"\n",
        "        )\n",
        "\n",
        "# Save model\n",
        "torch.save(net.state_dict(), \"model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8E3A5gi8zj-"
      },
      "outputs": [],
      "source": [
        "# Loading a model\n",
        "n_classes = 22\n",
        "model = UNet(n_classes)\n",
        "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
        "model = model.eval().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsDiU4G_8zj-"
      },
      "source": [
        "# III. Evalution of the Trained DNN on the test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djpEkMSF8zkB"
      },
      "source": [
        "## A. classical evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N11rmcci8zkC"
      },
      "outputs": [],
      "source": [
        "plot_losses(train_loss_history, val_loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qV-GoYE78zkC"
      },
      "outputs": [],
      "source": [
        "plot_accu(train_miou_history, val_miou_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3zDhHa_nCPr"
      },
      "source": [
        "**Q10/ Please plot the loss and MIoU. Comment.**\n",
        "\n",
        "Both the training and test loss curves show a similar downward trend, stabilizing around 0.3. However, there's a notable difference in the mean Intersection over Union (mIoU) values: the training mIoU reaches approximately 0.75, whereas the test mIoU plateaus at around 0.65. This discrepancy indicates a definite overfitting to the training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfAjk5J98zkC"
      },
      "source": [
        "**Q11/ What should we have done to avoid overfitting?**\n",
        "\n",
        "Overfitting often occurs when the model has too few training examples. One solution to avoid overfitting could be to train the model on more data either by increasing the dataset size with new real samples or by using data augmentation techniques such as applying linear transformation (like shift, rotatation or flip) to images for instance. Other techniques such as color shifting or adding noise might cause issue in the context of autonomous driving.\n",
        "\n",
        "Regularisation is also a good way to avoid overfitting. It can be viewed as simplifying the model in a random way and smaller models are less prone to overfitting.\n",
        "\n",
        "Early stopping is also a great idea. In this case, it could be use to stop the training as soon as the loss stabilises. However, one downside of early stopping is that any potential loss drop is out of reach (reaching a new local minimum after a couple of epochs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyOU5dc18zkD"
      },
      "outputs": [],
      "source": [
        "# Now we evaluate the model on all the test set.\n",
        "loss, iou, miou = test(model, test_loader, criterion, metric)\n",
        "print(f\"[FINAL TEST] loss={loss} | metric={miou}\")\n",
        "# Print per class IoU on last epoch or if best iou\n",
        "for muad_class, class_iou in zip(test_set.classes, iou, strict=True):\n",
        "    print(f\"[{muad_class.id}] {muad_class.name}: {class_iou:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7gPJw158zkD"
      },
      "source": [
        "## B. Uncertainty evaluations with MCP\n",
        "Here you will just use as confidence score the Maximum class probability (MCP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L4fmecp8zkD"
      },
      "outputs": [],
      "source": [
        "sample_idx = 0\n",
        "img, target = test_set[sample_idx]\n",
        "\n",
        "batch_img = img.unsqueeze(0).cuda()\n",
        "batch_target = target.unsqueeze(0).cuda()\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "\t# Forward propagation\n",
        "\toutputs = model(batch_img)\n",
        "\toutputs_proba = outputs.softmax(dim=1)\n",
        "\t# remove the batch dimension\n",
        "\toutputs_proba = outputs_proba.squeeze(0)\n",
        "\tconfidence, pred = outputs_proba.max(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afn94s1m8zkD"
      },
      "outputs": [],
      "source": [
        "# Undo normalization on the image and convert to uint8.\n",
        "mean = torch.tensor([0.485, 0.456, 0.406], device=img.device)\n",
        "std = torch.tensor([0.229, 0.224, 0.225], device=img.device)\n",
        "img = img * std[:, None, None] + mean[:, None, None]\n",
        "img = F.to_dtype(img, torch.uint8, scale=True)\n",
        "\n",
        "tmp_target = target.masked_fill(target == 255, 21)\n",
        "target_masks = tmp_target == torch.arange(22, device=target.device)[:, None, None]\n",
        "img_segmented = draw_segmentation_masks(img, target_masks, alpha=1, colors=test_set.color_palette)\n",
        "\n",
        "pred_masks = pred == torch.arange(22, device=pred.device)[:, None, None]\n",
        "pred_img = draw_segmentation_masks(img, pred_masks, alpha=1, colors=test_set.color_palette)\n",
        "\n",
        "img = F.to_pil_image(img)\n",
        "img_segmented = F.to_pil_image(img_segmented)\n",
        "confidence_img = F.to_pil_image(confidence)\n",
        "pred_img = F.to_pil_image(pred_img)\n",
        "\n",
        "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(30, 15))\n",
        "ax1.imshow(img)\n",
        "ax2.imshow(img_segmented)\n",
        "ax3.imshow(pred_img)\n",
        "ax4.imshow(confidence_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTLLCpTQ8zkD"
      },
      "source": [
        "**Q12/ The last image is the related to the confidence score of the DNN. Can you explain why? What does the birght areas represent and what does the dark areas represent?**\n",
        "\n",
        "From left to right, the first image is the input image and the second image is the target, that is to say the input image perfectly segmented. The third image represents the segmentation made by the model. For now, with only 2 training epochs, this result is very bad. The model did not notice the fence on the road nor did it notice any pedestrian. Only part of the car and of the road are correctly classified as well as some trees. On then last image, we can see the level of confidence of the model or in other words the probability with which it classify each pixel to its class. The brigth areas corresponds to areas where the model is very confident and dark areas represents low confidence. With the current model, the areas with the most confidence are the trees in the background, maybe not the most useful for autonomous driving..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2lBaYGD8zkD"
      },
      "source": [
        "### Now let's load the OOD test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EK6l_yi8zkE"
      },
      "outputs": [],
      "source": [
        "test_ood_set = MUAD(root=\"./data\", target_type=\"semantic\", version=\"small\", split=\"ood\" , transforms=val_transform, download=True)\n",
        "test_ood_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oonIF23g8zkE"
      },
      "outputs": [],
      "source": [
        "sample_idx = 0\n",
        "img, target = test_ood_set[sample_idx]\n",
        "\n",
        "batch_img = img.unsqueeze(0).cuda()\n",
        "batch_target = target.unsqueeze(0).cuda()\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "\t# Forward propagation\n",
        "\toutputs = model(batch_img)\n",
        "\toutputs_proba = outputs.softmax(dim=1)\n",
        "\t# remove the batch dimension\n",
        "\toutputs_proba = outputs_proba.squeeze(0)\n",
        "\tconfidence, pred = outputs_proba.max(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikr2WKe98zkE"
      },
      "outputs": [],
      "source": [
        "# Undo normalization on the image and convert to uint8.\n",
        "mean = torch.tensor([0.485, 0.456, 0.406], device=img.device)\n",
        "std = torch.tensor([0.229, 0.224, 0.225], device=img.device)\n",
        "img = img * std[:, None, None] + mean[:, None, None]\n",
        "img = F.to_dtype(img, torch.uint8, scale=True)\n",
        "\n",
        "tmp_target = target.masked_fill(target == 255, 21)\n",
        "target_masks = tmp_target == torch.arange(22, device=target.device)[:, None, None]\n",
        "img_segmented = draw_segmentation_masks(img, target_masks, alpha=1, colors=test_set.color_palette)\n",
        "\n",
        "pred_masks = pred == torch.arange(22, device=pred.device)[:, None, None]\n",
        "pred_img = draw_segmentation_masks(img, pred_masks, alpha=1, colors=test_set.color_palette)\n",
        "\n",
        "img_pil = F.to_pil_image(img)\n",
        "img_segmented = F.to_pil_image(img_segmented)\n",
        "confidence_img = F.to_pil_image(confidence)\n",
        "pred_img = F.to_pil_image(pred_img)\n",
        "\n",
        "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(30, 15))\n",
        "ax1.imshow(img_pil)\n",
        "ax2.imshow(img_segmented)\n",
        "ax3.imshow(pred_img)\n",
        "ax4.imshow(confidence_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOhtqIgm8zkE"
      },
      "source": [
        "**Q12 bis/ According to the output, is the model confident when it comes to labeling the bear and goat? How about the bench?**\n",
        "\n",
        "**Q12 ter/ The last image is the related to the confidence score of the DNN. Can you explain why? Are you happy with this image?**\n",
        "\n",
        "With 2 training epochs, the DNN is not confident on anything but the trees... However, I expect a better trained model to be very confident on the bench classified as bench but also confident on the bear and goat classified as undefined because there were none of these in the training set.\n",
        "\n",
        "Need to train the model for longer..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLGo9SKQ8zkE"
      },
      "source": [
        "## C. Uncertainty evaluations with Temperature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrPf8XueyEVo"
      },
      "source": [
        "**Q13/ Please implement a temperature scaling using torch_uncertainty**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R2towX48zkF"
      },
      "source": [
        "Before Temprature scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmxLKFIO8zkF"
      },
      "outputs": [],
      "source": [
        "# https://github.com/ENSTA-U2IS-AI/torch-uncertainty/blob/main/auto_tutorials_source/tutorial_scaler.py\n",
        "from torch_uncertainty.metrics import CalibrationError\n",
        "\n",
        "# Initialize the ECE\n",
        "ece = CalibrationError(task=\"multiclass\", num_classes=n_classes)\n",
        "\n",
        "# Iterate on the calibration dataloader\n",
        "for sample, target in test_loader:\n",
        "    logits = model(sample.cuda())\n",
        "    probs = logits.softmax(-1)\n",
        "    confidence, pred = probs.max(1)\n",
        "    confidence = torch.reshape(confidence, (confidence.shape[0], 1, confidence.shape[1], confidence.shape[2])).cpu()\n",
        "    ece.update(confidence, target)\n",
        "\n",
        "# Compute & print the calibration error\n",
        "print(f\"ECE before scaling - {ece.compute():.3%}.\")\n",
        "\n",
        "# Plot the top-label calibration figure\n",
        "fig, ax = ece.plot()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e8GJiWI8zkF"
      },
      "source": [
        "**Q13 bis/ Seeing the two graphs above comment on the MCP unceratinty result, is the model overconfident or calibrated ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKXrWIxo8zkF"
      },
      "source": [
        "After temperature scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v_bTeuX8zkF"
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty.post_processing import TemperatureScaler\n",
        "\n",
        "# Fit the scaler on the calibration dataset\n",
        "scaled_model = TemperatureScaler(model=model).cpu()\n",
        "scaled_model.fit(calibration_set=test_set) # Use OOD set???\n",
        "scaled_model = scaled_model.cuda()\n",
        "\n",
        "# Reset the ECE\n",
        "ece.reset()\n",
        "\n",
        "# Iterate on the calibration dataloader\n",
        "for sample, target in test_loader:\n",
        "    logits = scaled_model(sample.cuda())\n",
        "    probs = logits.softmax(-1)\n",
        "    confidence, pred = probs.max(1)\n",
        "    confidence = torch.reshape(confidence, (confidence.shape[0], 1, confidence.shape[1], confidence.shape[2])).cpu()\n",
        "    ece.update(confidence, target)\n",
        "\n",
        "# Compute & print the calibration error\n",
        "print(f\"ECE before scaling - {ece.compute():.3%}.\")\n",
        "\n",
        "# Plot the top-label calibration figure\n",
        "fig, ax = ece.plot()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_KLW4uf8zkF"
      },
      "source": [
        "Now let's see the new confidence score image after scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIXNjK3c8zkG"
      },
      "outputs": [],
      "source": [
        "sample_idx = 0\n",
        "img, target = test_set[sample_idx]\n",
        "\n",
        "batch_img = img.unsqueeze(0).cuda()\n",
        "batch_target = target.unsqueeze(0).cuda()\n",
        "scaled_model.eval()\n",
        "with torch.no_grad():\n",
        "\t# Forward propagation\n",
        "\toutputs = scaled_model(batch_img)\n",
        "\toutputs_proba = outputs.softmax(dim=1)\n",
        "\t# remove the batch dimension\n",
        "\toutputs_proba = outputs_proba.squeeze(0)\n",
        "\tconfidence, pred = outputs_proba.max(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-ORI-bdvPTF"
      },
      "outputs": [],
      "source": [
        "# Undo normalization on the image and convert to uint8.\n",
        "mean = torch.tensor([0.485, 0.456, 0.406], device=img.device)\n",
        "std = torch.tensor([0.229, 0.224, 0.225], device=img.device)\n",
        "img = img * std[:, None, None] + mean[:, None, None]\n",
        "img = F.to_dtype(img, torch.uint8, scale=True)\n",
        "\n",
        "tmp_target = target.masked_fill(target == 255, 21)\n",
        "target_masks = tmp_target == torch.arange(22, device=target.device)[:, None, None]\n",
        "img_segmented = draw_segmentation_masks(img, target_masks, alpha=1, colors=test_set.color_palette)\n",
        "\n",
        "pred_masks = pred == torch.arange(22, device=pred.device)[:, None, None]\n",
        "pred_img = draw_segmentation_masks(img, pred_masks, alpha=1, colors=test_set.color_palette)\n",
        "\n",
        "img_pil = F.to_pil_image(img)\n",
        "img_segmented = F.to_pil_image(img_segmented)\n",
        "confidence_img = F.to_pil_image(confidence)\n",
        "pred_img = F.to_pil_image(pred_img)\n",
        "\n",
        "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(30, 15))\n",
        "ax1.imshow(img_pil)\n",
        "ax2.imshow(img_segmented)\n",
        "ax3.imshow(pred_img)\n",
        "ax4.imshow(confidence_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4ZyTEsZ8zkG"
      },
      "source": [
        "**Q13 ter/ Did the model get more confident? Is it more calibrated? Comment on the temperature scaling graphs and results.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDWeuhIh8zkG"
      },
      "source": [
        "## D. Uncertainty evaluations with MC Dropout\n",
        "\n",
        "Let us implement **MC dropout**. This technique decribed in [this paper](https://arxiv.org/abs/1506.02142) allow us to have a better confindence score by using the dropout during test time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCcmyJ0V8zkG"
      },
      "source": [
        "**Q14/ Please implement MC Dropout using torch_uncertainty**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZQO-FDL8zkG"
      },
      "outputs": [],
      "source": [
        "# https://github.com/ENSTA-U2IS-AI/torch-uncertainty/blob/main/auto_tutorials_source/tutorial_mc_dropout.py\n",
        "from torch_uncertainty.models.wrappers.mc_dropout import mc_dropout\n",
        "from torch_uncertainty.routines import ClassificationRoutine\n",
        "from torch_uncertainty import TUTrainer\n",
        "\n",
        "mc_model = mc_dropout(model, num_estimators=16, last_layer=False).cuda()\n",
        "\n",
        "routine = ClassificationRoutine(\n",
        "    num_classes=n_classes,\n",
        "    model=mc_model,\n",
        "    loss=nn.CrossEntropyLoss(),\n",
        "    # optim_recipe=optim_cifar10_resnet18(mc_model), # ???\n",
        "    is_ensemble=True,\n",
        ")\n",
        "\n",
        "trainer = TUTrainer(accelerator=\"gpu\", max_epochs=2, enable_progress_bar=False)\n",
        "trainer.fit(model=routine, datamodule=train_loader)\n",
        "results = trainer.test(model=routine, datamodule=test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCXG19eYBwhL"
      },
      "outputs": [],
      "source": [
        "mc_model_path = \"mc.pth\"\n",
        "torch.save(ens_model.state_dict(), mc_model_path)\n",
        "print(f\"Model saved to {mc_model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_HBrewr2vMk"
      },
      "outputs": [],
      "source": [
        "sample_idx = 0\n",
        "img, target = test_set[sample_idx]\n",
        "\n",
        "batch_img = img.unsqueeze(0).cuda()\n",
        "batch_target = target.unsqueeze(0).cuda()\n",
        "mc_model.eval()\n",
        "with torch.no_grad():\n",
        "    # Forward propagation\n",
        "    outputs = mc_model(batch_img)\n",
        "    outputs_proba = outputs.softmax(dim=1)\n",
        "    # remove the batch dimension\n",
        "    outputs_proba = outputs_proba.squeeze(0)\n",
        "    max_outputs_proba, _ = outputs_proba.max(1)\n",
        "    confidence, pred = max_outputs_proba.max(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWqp2k5R2wUb"
      },
      "outputs": [],
      "source": [
        "# Undo normalization on the image and convert to uint8.\n",
        "mean = torch.tensor([0.485, 0.456, 0.406], device=img.device)\n",
        "std = torch.tensor([0.229, 0.224, 0.225], device=img.device)\n",
        "img = img * std[:, None, None] + mean[:, None, None]\n",
        "img = F.to_dtype(img, torch.uint8, scale=True)\n",
        "\n",
        "tmp_target = target.masked_fill(target == 255, 21)\n",
        "target_masks = tmp_target == torch.arange(22, device=target.device)[:, None, None]\n",
        "img_segmented = draw_segmentation_masks(img, target_masks, alpha=1, colors=test_set.color_palette)\n",
        "\n",
        "pred_masks = pred == torch.arange(22, device=pred.device)[:, None, None]\n",
        "pred_img = draw_segmentation_masks(img, pred_masks, alpha=1, colors=test_set.color_palette)\n",
        "\n",
        "img_pil = F.to_pil_image(img)\n",
        "img_segmented = F.to_pil_image(img_segmented)\n",
        "confidence_img = F.to_pil_image(confidence)\n",
        "pred_img = F.to_pil_image(pred_img)\n",
        "\n",
        "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(30, 15))\n",
        "ax1.imshow(img_pil)\n",
        "ax2.imshow(img_segmented)\n",
        "ax3.imshow(pred_img)\n",
        "ax4.imshow(confidence_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqeweZaq8zkH"
      },
      "source": [
        "**Q14 bis/ Try the MC dropout code with a low number of estimators T like 3 and a high number 20, Explain the diffrence seen on the confidence image, is the model getting more confident or less ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzHxIgy20VUj"
      },
      "source": [
        "## E. Uncertainty evaluations with Deep Ensembles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faf7ifKb8zkH"
      },
      "source": [
        "**Q15/ Please implement [Deep Ensembles](https://papers.nips.cc/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf).**\n",
        "\n",
        "\n",
        "1.   You need to train 3 DNNs and save it. (Go back to the training cell above and train and save 3 diffrent models)\n",
        "2.   Use TorchUncertainty to get predictions\n",
        "\n",
        "You have two options either train several models using the code above or use TU to train the ensemble of models in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcUk8mfK8zkH"
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty.models import deep_ensembles\n",
        "from torch_uncertainty.routines import ClassificationRoutine\n",
        "from torch_uncertainty import TUTrainer\n",
        "\n",
        "ens_model = deep_ensembles(model, num_estimators=16).cuda()\n",
        "\n",
        "ens_routine = ClassificationRoutine(\n",
        "    num_classes=n_classes,\n",
        "    model=ens_model,\n",
        "    loss=nn.CrossEntropyLoss(),\n",
        "    # optim_recipe=optim_cifar10_resnet18(mc_model), # ???\n",
        "    is_ensemble=True,\n",
        ")\n",
        "\n",
        "ens_trainer = TUTrainer(accelerator=\"gpu\", max_epochs=2, enable_progress_bar=False)\n",
        "ens_trainer.fit(model=ens_routine, datamodule=train_loader)\n",
        "results = trainer.test(ens_routine, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8Tc7bav8zkJ"
      },
      "source": [
        "Save the ensemble model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ9y8Woz8zkJ"
      },
      "outputs": [],
      "source": [
        "ens_model_path = \"ensemble.pth\"\n",
        "torch.save(ens_model.state_dict(), ens_model_path)\n",
        "print(f\"Model saved to {ens_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESqKoBPG0a2A"
      },
      "source": [
        "## F. Uncertainty evaluations with Packed-Ensembles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZf8B6xf8zkJ"
      },
      "source": [
        "**Q16/ Please read [Packed-Ensembles](https://arxiv.org/pdf/2210.09184). Then Implement a Packed-Ensembles Unet and train it and evaluate its Uncertainty.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74wEu5jG8zkJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eh9Uruml8zkJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfKOdgTv8zkK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vERbFaOf8zkK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0txU0p_f8zkK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czg42bYrRcG7"
      },
      "source": [
        "**Please conclude your report.**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d94a0a0e0d540d0841a29fb60122e7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "152d79f29c4c4cfdbb26e16e1a530b83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35f7068931aa4f6687d6de866f16d55a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8adce99bd93e417b9eeeec7d0981bbbf",
            "placeholder": "​",
            "style": "IPY_MODEL_adc183b26a87432ab9c28a4c8fac2df7",
            "value": " 1.06G/1.06G [00:25&lt;00:00, 42.3MB/s]"
          }
        },
        "3fd6d7a73aeb4aabb9f103d482753ba5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56c1a7855c4d4163bc3e4ad0bb503898": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c4e318aa6fd43bbbc21cc67dd3f139c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6bdc7a31482041e595b67c4153f28b29": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e364c9177a54618944edad4ae8b40d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73f9e4ca64024f25bae74a5e924a9f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "790520797f32449c8a6b27edde85f5c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3a8d6d9eb3543d48ac1b5d7d6f88003",
            "placeholder": "​",
            "style": "IPY_MODEL_7d10b247927b4cfc9ab31413f79fdedc",
            "value": "val.zip: 100%"
          }
        },
        "7b40dd48f7cd4beab8125638f592d1ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b6ea7859ce64f9b8b0f450e68781405": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be073e0667e5437e8968d298b2f2fcca",
            "placeholder": "​",
            "style": "IPY_MODEL_d52357b30b5247d1aad718493ad53fac",
            "value": " 511M/511M [00:11&lt;00:00, 42.7MB/s]"
          }
        },
        "7c4c90cbbf154109be1e6976efae0b6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d10b247927b4cfc9ab31413f79fdedc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80b2eeb6e2d144b280012dfee12c69fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "861df32105834f6294b844b55d2fc940": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8adce99bd93e417b9eeeec7d0981bbbf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9208a5b5146a4f7486a3457471efcbc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_790520797f32449c8a6b27edde85f5c7",
              "IPY_MODEL_aaa51c3b805f4716bf0871d2332b9341",
              "IPY_MODEL_7b6ea7859ce64f9b8b0f450e68781405"
            ],
            "layout": "IPY_MODEL_6e364c9177a54618944edad4ae8b40d5"
          }
        },
        "98cc66c85f9244d3bb5e1e426a640cad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cdbef89c3ca4f2980c7d751b87c9fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80b2eeb6e2d144b280012dfee12c69fe",
            "placeholder": "​",
            "style": "IPY_MODEL_7c4c90cbbf154109be1e6976efae0b6f",
            "value": " 3.83G/3.83G [01:30&lt;00:00, 42.2MB/s]"
          }
        },
        "a4d307edd2834f72b5c2d4eff21042d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f97aa393955043a1810f7c3dc807a9fe",
              "IPY_MODEL_f377338154384c08a9b620857cc1fecf",
              "IPY_MODEL_35f7068931aa4f6687d6de866f16d55a"
            ],
            "layout": "IPY_MODEL_3fd6d7a73aeb4aabb9f103d482753ba5"
          }
        },
        "a8f05cc5a1f346be9145fc41b17bf126": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bdc7a31482041e595b67c4153f28b29",
            "placeholder": "​",
            "style": "IPY_MODEL_73f9e4ca64024f25bae74a5e924a9f18",
            "value": "train.zip: 100%"
          }
        },
        "aaa51c3b805f4716bf0871d2332b9341": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f54ed6337d8746fb8e89665502fbdec8",
            "max": 511251992,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_152d79f29c4c4cfdbb26e16e1a530b83",
            "value": 511251992
          }
        },
        "adc183b26a87432ab9c28a4c8fac2df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3a8d6d9eb3543d48ac1b5d7d6f88003": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7d52bf1bf494b96946d805b27710e69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be073e0667e5437e8968d298b2f2fcca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d52357b30b5247d1aad718493ad53fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7ac31001fce42f9a6c1742b2f0f817a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8f05cc5a1f346be9145fc41b17bf126",
              "IPY_MODEL_f8cdcc7bce40484d908b90e6f90649b5",
              "IPY_MODEL_9cdbef89c3ca4f2980c7d751b87c9fd1"
            ],
            "layout": "IPY_MODEL_861df32105834f6294b844b55d2fc940"
          }
        },
        "f377338154384c08a9b620857cc1fecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98cc66c85f9244d3bb5e1e426a640cad",
            "max": 1063029826,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56c1a7855c4d4163bc3e4ad0bb503898",
            "value": 1063029826
          }
        },
        "f54ed6337d8746fb8e89665502fbdec8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8cdcc7bce40484d908b90e6f90649b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7d52bf1bf494b96946d805b27710e69",
            "max": 3827406962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c4e318aa6fd43bbbc21cc67dd3f139c",
            "value": 3827406962
          }
        },
        "f97aa393955043a1810f7c3dc807a9fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d94a0a0e0d540d0841a29fb60122e7b",
            "placeholder": "​",
            "style": "IPY_MODEL_7b40dd48f7cd4beab8125638f592d1ed",
            "value": "test.zip: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
